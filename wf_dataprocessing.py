def process():
    # -*- coding: utf-8 -*-
    """ds594.ipynb

    Automatically generated by Colaboratory.

    Original file is located at
        https://colab.research.google.com/drive/1PR6wk_eajU7yJ1AKNoElysThllGyThNX
    """

    import pandas as pd
    names = ['target', 'id', 'date', 'flag', 'user', 'text']
    df = pd.read_csv('data_original.csv', encoding='ISO-8859-1', header=None, names=names)
    # df_clean =

    df

    df.nunique()

    df.drop_duplicates(subset=['id'], inplace=True)

    df.drop_duplicates(subset=['text'], inplace=True)

    df.drop(df[df['user'] == "tweetpet"].index, inplace=True)

    df

    df.drop(axis=1, columns=['id', 'flag'], inplace=True)

    df

    df.text.str.lower()

    df['text'] = df.text.str.lower()

    df

    df1 = df.loc[:, ['user']]
    df1 = df1['user'].value_counts().to_dict()

    users = []
    for i in df1:
        if df1[i] >= 100:
            users.append(i)

    len(users)

    user_greater100 = df.apply(lambda row: row[df['user'].isin(users)])

    user_greater100

    df.isnull().any()

    df = df.apply(lambda row: row[df['user'].isin(users)])

    df3 = df.loc[:, ['text']]

    def preprocessing(reviews):
        # Import nltk - only use nltk library to perform all the following processing.
        import nltk
        """
        :param reviews: Reviews list
        :return: Dataframe with processed reviews

        Lower-case all words.
        Remove all punctuations.
        Remove stopwords. (Stopwords are the lists in the nltk library that are trivial and not relevant to the context/text.)
        Perform lemmatization on the data.
        """
        # TODO
        cleaned_review = reviews.copy()
        from nltk.corpus import stopwords
        nltk.download('stopwords')
        nltk.download('wordnet')
        nltk.download('omw-1.4')
        nltk.download('punkt')
        from nltk.stem import WordNetLemmatizer
        from nltk.tokenize import word_tokenize

        reviewsList = cleaned_review['review']

        cleaned_review['review'] = cleaned_review['review'].str.lower()

        cleaned_review['review'] = cleaned_review['review'].replace(r'http\S+', '', regex=True).replace(r'www\S+', '',
                                                                                                        regex=True)
        cleaned_review['review'] = cleaned_review['review'].str.replace(r'@\w+', " ")

        cleaned_review['review'] = cleaned_review['review'].str.replace(r'[^\w\s]+', " ")

        lemmatizer = WordNetLemmatizer()
        for i in range(len(reviewsList)):
            review = cleaned_review['review'][i]
            review = [i for i in word_tokenize(review) if i not in stopwords.words('english')]
            cleaned_review['review'][i] = review

            lemmatized_words = []
            for word in review:
                lemmatized_words.append(lemmatizer.lemmatize(word))
            cleaned_review['review'][i] = " ".join(lemmatized_words)

        # Return the dataframe with the processed data
        return cleaned_review  # TODO

    df['text']

    # In[24]:

    data = df.to_numpy()

    # In[25]:

    data

    # In[26]:

    x = data[:, 3]

    # In[27]:

    df3 = pd.DataFrame(x, columns=['review'])

    ans = preprocessing(df3)

    ans

    df['text'] = ans

    df

    ans = ans.rename(columns={"review": "text"})

    df['text'] = ans

    xx = df[df.text.isna()]

    data[:, 3] = ans['text']

    len(data[:, 3])

    aaa = pd.DataFrame(data=data, columns=["target", "date", "user", "text"])

    aaa

    y = aaa[aaa.text.isna()]

    y

    file_name = "out.csv"

    aaa.drop_duplicates(subset=['text'], inplace=True)

    aaa.drop(aaa[aaa['user'] == ""].index, inplace=True)

    aaa.to_csv(file_name, encoding='utf-8')

